Data Science for Business
by Foster Provost and Tom Fawcett
Published by O’Reilly Media, Inc.

* Chapter 2. Business Problems and Data Science Solutions
** From Business Problems to Data Mining Tasks  :drill:
A critical skill in data science is the ability to [decompose] a data-
analytics problem into pieces such that each piece matches a known
task for which tools are available. Recognizing [familiar] problems and
their solutions avoids wasting time and resources reinventing the wheel.

Despite the large number of specific data mining algorithms developed over
the years, there are only a handful of fundamentally different types of
tasks these algorithms address. It is worth defining these tasks clearly.

*** TODO Table - Unfinished

| Task                                            | Desc |
|-------------------------------------------------+------|
| Classification and class probability estimation |      |
| Regression (“value estimation”)                 |      |
| Similarity                                      |      |
| Similarity Matching                             |      |
| Clustering                                      |      |
| Co-occurance Grouping                           |      |
| Profiling                                       |      |
| Link Prediction                                 |      |
| Data Reduction                                  |      |
| Causal Modeling                                 |      |

*** Drill  :drill:
[Classification] and [class probability estimation] attempt to predict,
for each individual in a population, which of a (small) set of [classes]
this individual belongs to. Usually the [classes] are mutually exclusive.

*** Drill  :drill:
[Classification] and [class probability estimation] attempt to [predict],
for each individual in a population, which of a (small) set of classes
this individual belongs to. Usually the classes are [mutually exclusive].

*** Drill  :drill:
An example classification question:  “Among all the customers which
are likely to respond to a given offer?” In this example the [two]
classes could be called ["will respond"] and ["will not" respond].

*** Drill  :drill:
A closely related task is scoring or class [probability estimation]. 
A scoring model applied to an individual produces, instead
of a class prediction, a score representing the [probability] (or some
other quantification of likelihood) that that individual belongs to
each class. In our customer response scenario, a scoring model would
be able to evaluate each individual customer and produce a score of
how likely each is to respond to the offer.

*** Drill  :drill:
Regression (or “value estimation”) attempts to [estimate] or predict, for 
each individual, the [numerical] value of some variable for that individual.

An example regression question would be: “How [much] will a given customer
use the service?” The property (variable) to be predicted here is
service usage, and a model could be generated by looking at other,
similar individuals in the population and their historical usage.

*** Drill  :drill:
Informally, [classification] predicts whether something will happen, whereas
[regression] predicts how much something will happen.

*** Drill  :drill:
Similarity matching attempts to identify [similar] individuals based
on data known about them.

*** Drill  :drill:
[Similarity matching] is the basis for one of the most popular methods
for making product recommendations (finding people who are similar
to you in terms of the products they have liked or have purchased)

*** Drill  :drill:
[Clustering] attempts to group individuals in a population together by
their similarity, but not driven by any specific [purpose]. An example
[clustering] question would be: “Do our customers form natural groups
or segments?”

*** Drill  :drill:
[Co-occurrence grouping] (also known as frequent itemset mining, association
rule discovery, and market-[basket] analysis) attempts to find associations
between entities based on transactions involving them. An example [co-
occurrence] question would be: What items are commonly purchased together?

*** Drill  :drill:
[Clustering] looks at similarity between objects based on the objects’
[attributes], while [co-occurrence grouping] considers similarity of objects
based on their appearing together in transactions.

*** Drill  :drill:
Co-occurrence of products in purchases is a common type of grouping
known as [market-basket analysis].

*** Drill  :drill:
Profiling (also known as [behavior] description) attempts to characterize
the typical [behavior] of an individual, group, or population.

*** Drill  :drill:
[Profiling] is often used to establish behavioral norms for [anomaly]
detection applications such as [fraud] detection and monitoring for 
[intrusions] to computer systems.

*** Drill  :drill:
[Link prediction] attempts to predict connections between data items,
usually by suggesting that a [link] should exist, and possibly also
estimating the strength of the [link]. [Link prediction] is common in social
networking systems.

*** Drill  :drill:
Link prediction can also estimate the [strength] of a link. We search
for links that do not exist between customers and movies, but that
we predict should [exist] and should be [strong].  These links form the
basis for [recommendations].

*** Drill  :drill:
[Data reduction] attempts to take a large set of data and replace it
with a smaller set of data that contains much of the important [information]
in the larger set. The smaller dataset may be easier to deal with or
to process. Moreover, the smaller dataset may [better reveal information].

*** Drill  :drill:
[Causal modeling] attempts to help us understand what events or actions
actually influence others. For example, did the advertisements influence
consumers to purchase? Or did the predictive models simply do a good
job of identifying those consumers who would have purchased anyway?

*** Drill  :drill:
Techniques for causal modeling include those involving a substantial
investment in data, such as [randomized] controlled experiments (eg,
so-called [“A/B tests”]), as well as sophisticated methods for drawing 
causal conclusions from [observational] data.

** Supervised Versus Unsupervised Methods  :drill:
The terms supervised and unsupervised were inherited from the field
of [machine learning]. Metaphorically, a teacher “supervises” the learner
by carefully providing [target] information along with a set of examples.
An [unsupervised] learning task might involve the same set of examples but
would not include the [target] information. The learner would be given
no information about the [purpose] of the learning, but would be left
to form its own conclusions about what the examples have in common.

*** Drill  :drill:
Technically, another condition must be met for supervised data mining:
there must be [data on the target]. It is not enough that the [target]
information exist in principle; it must also exist in the [data].

*** Drill  :drill:
[Supervised] tasks require different techniques than unsupervised tasks do,
and the results often are much more [useful]. A supervised technique is
given a [specific purpose] for the grouping—predicting the target.
[Clustering], an unsupervised task, produces [groupings] based on
similarities, but there is no guarantee that these similarities
are [meaningful] or will be useful for any particular [purpose].

*** Drill  :drill:
Acquiring data on the [target] often is a key data science [investment].
The value for the target variable for an individual is often called
the individual’s [label], emphasizing that often (not always) one
must incur expense to actively [label] the [data]. 

*** Drill  :drill:
Classification, regression, and causal modeling generally are solved 
with [supervised] methods.  Similarity matching, link prediction, and 
data reduction could be [either]. Clustering, co-occurrence grouping, and
profiling generally are [unsupervised]. The fundamental principles of
data mining that we will present underlie all these types of technique.

*** Drill  :drill:
[Classification], [regression], and [causal modeling] generally are solved 
with supervised methods.  

*** Drill  :drill:
[Similarity matching], [link prediction], and [data reduction] can be
solved with either supervised or [unsupervised] methods. 

*** Drill  :drill:
[Clustering], [co-occurrence grouping], and [profiling] generally use
unsupervised methods.

*** Drill  :drill:
Classification, regression, and causal modeling generally are solved 
with [supervised] methods.  

*** Drill  :drill:
Similarity matching, link prediction, and data reduction can be
solved with [either supervised or unsupervised] methods. 

*** Drill  :drill:
Clustering, co-occurrence grouping, and profiling generally use
[unsupervised methods].

*** Drill  :drill:
Two main subclasses of [supervised learning], classification and regression,
are distinguished by the type of target. Regression involves a [numeric] 
target while classification involves a [categorical (often binary)] target.

*** Drill  :drill:
Two main subclasses of supervised learning, [classification] and [regression],
are distinguished by the type of target. [Regression] involves a numeric 
target while [classification] involves a categorical (often binary) target.

*** Drill  :drill:
For business applications we often want a [numerical prediction] over a 
[categorical target]. In the churn example, a basic yes/no prediction of 
whether a customer is likely to continue to subscribe to the service 
may not be sufficient; we want to model the [probability] that the customer 
will continue. This is still considered [classification] modeling rather 
than [regression] because the underlying target is [categorical]. Where 
necessary for clarity, this is called [“class probability estimation.”]

*** Drill  :drill:
A vital part in the early stages of the data mining process is (i) to 
decide whether the line of attack will be [supervised] or [unsupervised], 
and (ii) if [supervised], to produce a precise definition of a [target]
variable. This variable must be a specific [quantity] that will be the 
focus of the data mining.

** The Data Mining Process
*** Drill  :drill:
Data science is a [craft].  As with many mature crafts, there is a
 well-understood [process] that places a structure on the problem, 
allowing reasonable [consistency], [repeatability], and [objectiveness].

*** Drill  :drill:
A useful codification of the data mining process is given by the
 [Cross Industry Standard Process for Data Mining] (CRISP-DM).

*** Drill  :drill:
A useful codification of the data mining process is [CRISP-DM] (abbv).

*** TODO - Add illustration Figure 2-2.1 Figure 2-2. 

*** CRISP-DM Table  :drill:
:PROPERTIES:
:DRILL_CARD_TYPE: hide2cloze
:END:

The CRISP-DM process includes the following stages:

1. [Business Understanding]
2. [Data Understanding]
3. [Data Preparation]
4. [Modeling]
5. [Evaluation]
6. [Deployment]

*** About the CRISP data mining process  :drill:
:PROPERTIES:
:DRILL_CARD_TYPE: hide2cloze
:END:

This process diagram makes explicit the fact that [iteration] is the rule 
rather than the exception. Going through the process once without having 
solved the problem is, generally speaking, not a [failure]. Often the 
entire process is an [exploration] of the data, and after the first 
[iteration] the data science team [knows much more].  The next iteration 
can be much more [well-informed]. 

** Business Understanding  :drill:
Initially, it is vital to understand the problem to be solved. 
This may seem obvious, but business projects seldom come pre-packaged 
as clear and unambiguous data mining problems.  Often, [recasting] the 
problem and [designing] a solution is an [iterative] process of [discovery].

*** Drill  :drill: 
The [Business Understanding] stage represents a part of the craft
where the analysts’ creativity plays a large role. Data science 
has some things to say, as we will describe, but often the key 
to a great success is a creative problem formulation by some analyst 
regarding how to cast the [business] problem as one or more [data 
science] problems. High-level knowledge of the [fundamentals] helps 
creative business analysts see novel formulations.

** Data Understanding  :drill:
A critical part of the [data understanding] phase is estimating 
the [costs] and [benefits] of each [data source] and deciding 
whether further [investment] is merited.

*** Credit card fraud  :drill:
  :PROPERTIES:
  :DRILL_CARD_TYPE: hide2cloze
  :END:
Data mining has been used extensively for fraud detection, and 
many fraud detection problems involve classic [supervised] data 
mining tasks. Fraudulent charges are usually caught—if not initially 
by the [company], then later by the [customer] when account activity 
is reviewed. Nearly all fraud is [identified] and reliably 
[labeled], since the legitimate customer and the person perpetrating 
the fraud are different people and have opposite goals. Thus credit 
card transactions have reliable [labels] (fraud and legitimate) that 
may serve as targets for a [supervised] technique. 

*** Medicare fraud  :drill:
  :PROPERTIES:
  :DRILL_CARD_TYPE: hide2cloze
  :END:
Those who commit [Medicare] fraud are a subset of the legitimate 
users; there is no separate disinterested party who will declare 
exactly what the “correct” charges should be. Consequently the 
Medicare billing data have no reliable [target] variable indicating 
fraud, and a [supervised] learning approach that could work for 
credit card fraud is not applicable. Such a problem usually requires 
[unsupervised] approaches such as [profiling], [clustering], [anomaly 
detection], and [co-occurrence grouping].

** Data Preparation  :drill:
Typical examples of data [preparation] are converting data to tabular 
format, removing or [inferring missing] values, and converting data 
to different [types]. Some data mining techniques are designed for 
[symbolic] and [categorical] data, while others handle only [numeric] 
values. In addition, numerical values must often be [normalized] 
or [scaled] so that they are comparable. 

*** Leaks  :drill:
One very general and important concern during data preparation 
is to beware of [“leaks”] (Kaufman et al. 2012). A [leak] is a situation 
where a [variable] collected in historical data gives information 
on the [target] variable — information that appears in historical 
data but is not actually available when the decision has to be made. 

** Modeling  :drill:
The output of the [modeling] stage is some sort of [model] or pattern 
capturing regularities in the data.

** Evaluation  :drill:
The purpose of the [evaluation] stage is to assess the data mining 
results rigorously and to gain confidence that they are valid 
and reliable before moving on. If we look hard enough at any dataset 
we will find [patterns], but they may not survive careful scrutiny. 

*** Evaluation State and Business Goals  :drill:
Equally important, the evaluation stage also serves to help ensure 
that the model satisfies the original [business] goals. Recall 
that the primary goal of data science for business is to support
[decision making], and that we started the process by focusing 
on the business problem we would like to solve.

*** Drill  :drill:
To facilitate such qualitative assessment, the data scientist 
must think about the comprehensibility of the model to [stakeholders] 
(not just to the [data scientists]). And if the model itself is not 
comprehensible (e.g., maybe the model is a very complex mathematical 
formula), how can the data scientists work to make the behavior 
of the model be comprehensible.

*** Evaluation framework  :drill:
A comprehensive evaluation [framework] is important because 
getting detailed information on the [performance] of a deployed 
model may be difficult or impossible.

*** A/B Testing  :drill:
In our churn example, if we have decided from laboratory tests that
a data mined model will give us better churn reduction, we may want 
to move on to an [“in vivo”] evaluation, in which a live system 
randomly applies the model to some customers while keeping other 
customers as a [control group].

** Deployment  :drill:
In the [deployment] stage the results of data mining, and increasingly 
the data mining techniques themselves, are put into real use in order 
to realize some return on investment.

*** Deploying data mining models  :drill:
Increasingly, the data mining techniques themselves are deployed. 
For example, for targeting online advertisements, systems are 
deployed that automatically build (and test) [models] in [production] 
when a new advertising campaign is presented.

*** Deploying data mining models  :drill:
Two main reasons for deploying the data mining system itself rather than 
the models produced by a data mining system are (i) the world may [change
faster] than the data science team can adapt, as with fraud and intrusion 
detection, and (ii) a business has too many [modeling] tasks for their 
data science team to manually curate each [model] individually. 

*** Deploying into production  :drill:
  :PROPERTIES:
  :DRILL_CARD_TYPE: show2cloze
  :END:
It may be best to deploy the data mining phase into production. In doing 
so, it is critical to [instrument] the system to alert the data science
team of any seeming [anomalies] and to provide [fail-safe] operation.

** Implications for Managing the Data Science Team  :drill:
  :PROPERTIES:
  :DRILL_CARD_TYPE: hide2cloze
  :END:
Data mining is an [exploratory] undertaking closer to [research and 
development] than it is to [engineering]. The [CRISP] cycle is based 
around [exploration]; it iterates on approaches and strategy rather 
than on [software designs]. Outcomes are far less [certain], and the 
results of a given step may change the understanding of the problem. 

** Software skills versus analytics skills  :drill:
In analytics, it’s important for individuals to be able to formulate 
problems well, to [prototype] solutions quickly, to make good [assumptions] 
in the face of ill-structured problems, to design [experiments]
that represent good [investments], and to analyze results.

** Other Analytics Techniques and Technologies
  :PROPERTIES:
  :DRILL_CARD_TYPE: show2cloze
  :END:
To this end, we present six groups of related analytic techniques. 
Where appropriate we draw comparisons and contrasts with data mining. 
The main difference is that data mining focuses on the [automated] 
search for [knowledge], [patterns], or [regularities] from [data].

*** Statistics  :drill:
  :PROPERTIES:
  :DRILL_CARD_TYPE: show2cloze
  :END:
Often we want to calculate [summary statistics] [conditionally] on one 
or more [subsets] of the [population] (e.g., “Does the churn rate differ 
between male and female customers?”

*** Database Querying  :drill:
  :PROPERTIES:
  :DRILL_CARD_TYPE: show2cloze
  :END:
A [query] is a specific request for a [subset] of data or for [statistics] 
about data, formulated in a technical language and posed to a [database] 
system.  

*** Other Analytics Techniques and Technologies  :drill:
[On-line Analytical Processing] (OLAP) provides an easy-to-use GUI 
to query large data collections, for the purpose of facilitating 
data exploration. The idea of [“on-line” processing] is that it 
is done in realtime, so analysts and decision makers can find 
answers to their queries quickly and efficiently. Unlike the [“ad 
hoc”] querying enabled by tools like SQL, for OLAP the dimensions 
of analysis must be pre-programmed into the OLAP system. 

**** Drill  :drill:
OLAP systems are designed to facilitate manual or visual exploration 
of the data by analysts. OLAP performs no modeling or automatic 
[pattern finding]. As an additional contrast, unlike with OLAP, [data 
mining] tools generally can incorporate new dimensions of analysis 
easily as part of the exploration. OLAP tools can be a useful 
[complement] to data mining tools for discovery from business data.

*** Data Warehousing  :drill:
[Data warehouses] collect and coalesce data from across an enterprise, 
often from multiple transaction-processing systems, each with 
its own database.

**** Drill  :drill:
Data [warehousing] may be seen as a facilitating technology of data 
mining. It is not always necessary, as most data mining does not 
access a [data warehouse], but firms that decide to invest in [data 
warehouses] often can apply data mining more broadly and more deeply 
in the organization.

*** Regression Analysis  :drill:
  :PROPERTIES:
  :DRILL_CARD_TYPE: hide2cloze
  :END:
This book will focus on different issues than usually encountered 
in a regression analysis book or class. Here we are less interested 
in explaining a particular [dataset] as we are in extracting [patterns] 
that will [generalize] to other data, and for the purpose of improving 
some business process. Typically, this will involve [estimating] or 
[predicting] values for cases that are not in the analyzed data set. 

**** Explanatory Models vs Predictive Modeling  :drill:
The topic of explanatory modeling versus predictive modeling can elicit 
deep-felt debate, which goes well beyond our focus. What is important 
is to realize that there is considerable overlap in the [techniques] used,
but that the lessons learned from [explanatory] modeling do not all apply 
to [predictive] modeling. A reader with some background in regression 
analysis may encounter new and even seemingly contradictory lessons.

*** Machine Learning and Data Mining  :drill:
The collection of methods for extracting [predictive] models from 
data, now known as [machine learning] methods, were developed in 
several fields contemporaneously, most notably [Machine Learning], 
Applied Statistics, and Pattern Recognition.

**** Drill  :drill: 
Machine Learning as a field of study arose as a subfield of [Artificial 
Intelligence], which was concerned with methods for improving the 
knowledge or performance of an intelligent [agent] over time, in response 
to the [agent’s] experience in the world. 

**** Drill  :drill: 
[Machine Learning] as a field of study arose as a subfield of AI.

**** Drill  :drill:
The field of Data Mining (or [KDD]: [Knowledge Discovery and Data Mining]) 
started as an offshoot of [Machine Learning], and they remain closely 
linked. Both fields are concerned with the [analysis] of data to find 
useful or informative [patterns]. Indeed, the areas are so closely related 
that researchers commonly participate in both communities.

**** Drill :drill:
  :PROPERTIES:
  :DRILL_CARD_TYPE: hide2cloze
  :END:
Because [Machine Learning] is concerned with many types of performance
improvement, it includes subfields such as [robotics] and [computer vision]
that are not part of [KDD]. It also is concerned with issues of agency
and cognition, how will an intelligent agent use learned knowledge to
reason and act in an environment, which are not concerns of [Data Mining].

**** Drill  :drill:
  :PROPERTIES:
  :DRILL_CARD_TYPE: hide2cloze
  :END:
Historically, KDD spun off from Machine Learning as a research 
field focused on concerns raised by examining [real-world] applications, 
and a decade and a half later the KDD community remains more concerned 
with applications than Machine Learning is. As such, research 
focused on [commercial] applications and [business] issues of data 
analysis tends to gravitate toward the [KDD] community rather than 
to [Machine Learning]. [KDD] also tends to be more concerned with 
the entire process of data analytics: data preparation, model 
learning, evaluation, and so on.

